{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Functions\" data-toc-modified-id=\"Functions-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Fit-&amp;-Predict-Models\" data-toc-modified-id=\"Fit-&amp;-Predict-Models-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Fit &amp; Predict Models</a></span></li><li><span><a href=\"#Plot-Normalized-Confusion-Matrices\" data-toc-modified-id=\"Plot-Normalized-Confusion-Matrices-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Plot Normalized Confusion Matrices</a></span></li><li><span><a href=\"#Print-Classification-Reports\" data-toc-modified-id=\"Print-Classification-Reports-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Print Classification Reports</a></span></li><li><span><a href=\"#Plot-ROC-Curves\" data-toc-modified-id=\"Plot-ROC-Curves-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Plot ROC Curves</a></span></li><li><span><a href=\"#Accuracy-Scores\" data-toc-modified-id=\"Accuracy-Scores-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Accuracy Scores</a></span></li><li><span><a href=\"#Feature-Importance-Plot\" data-toc-modified-id=\"Feature-Importance-Plot-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Feature Importance Plot</a></span></li><li><span><a href=\"#Grid-Search\" data-toc-modified-id=\"Grid-Search-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>Grid Search</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit & Predict Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fit & Predict Models ###\n",
    "\n",
    "# Define function to fit our models and predict on the training and test sets\n",
    "\n",
    "def clf_pred(models):\n",
    "    '''\n",
    "    Intakes our DataFrame with our models \n",
    "        and our X & y, train & test data\n",
    "    \n",
    "    Returns a DataFrame with all of our models, \n",
    "        metrics and accuracy scores   \n",
    "    '''\n",
    "    \n",
    "    # Initialize matrix to fill\n",
    "    clf_df = np.zeros((len(models), 26), dtype=object)\n",
    "    \n",
    "    for i,model in models.iterrows():\n",
    "        \n",
    "\n",
    "        # Classifier Names & Models\n",
    "        clf_df[i,0] = model['clf_name']\n",
    "        clf_df[i,1] = model['clfs'] \n",
    "        \n",
    "        ### Assign Variables ###\n",
    "        # We do this here because XGBoost takes \n",
    "        # np.array format unlike the other models\n",
    "        X_train = model['X_train']\n",
    "        X_test = model['X_test']\n",
    "        y_train = model['y_train']\n",
    "        y_test = model['y_test']\n",
    "\n",
    "        ### Initialize Timer ### \n",
    "        start_time = time.time()\n",
    "        \n",
    "        ### Fit & Predict ###\n",
    "    \n",
    "        # Fit Model\n",
    "        fitted = model['clfs'].fit(X_train, y_train)\n",
    "        clf_df[i,2] = fitted\n",
    "        \n",
    "        # Calculate time to fit model\n",
    "        stop_time = time.time()\n",
    "        runtime = (stop_time - start_time)\n",
    "        clf_df[i,3] = runtime\n",
    "    \n",
    "        # Predict\n",
    "        y_pred_train = fitted.predict(X_train)\n",
    "        y_pred_test = fitted.predict(X_test)\n",
    "\n",
    "        clf_df[i,4] = y_pred_train\n",
    "        clf_df[i,5] = y_pred_test\n",
    "    \n",
    "        # y_score\n",
    "        y_score_train = fitted.predict_proba(X_train)\n",
    "        y_score_test = fitted.predict_proba(X_test)\n",
    "        clf_df[i,6] = y_score_train\n",
    "        clf_df[i,7] = y_score_test\n",
    "    \n",
    "        # False & True Positive Rates\n",
    "        clf_df[i,8], clf_df[i,9], thresholds_train = roc_curve(y_train, y_score_train[:,1])\n",
    "        clf_df[i,10], clf_df[i,11], thresholds_test = roc_curve(y_test, y_score_test[:,1])\n",
    "        \n",
    "        \n",
    "        ### Accuracy Scores ### \n",
    "        \n",
    "        # Precision\n",
    "        clf_df[i,12] = precision_score(y_train, y_pred_train)\n",
    "        clf_df[i,13] = precision_score(y_test, y_pred_test)\n",
    "        \n",
    "        # Recall \n",
    "        clf_df[i,14] = recall_score(y_train, y_pred_train)\n",
    "        clf_df[i,15] = recall_score(y_test, y_pred_test)        \n",
    "        \n",
    "        # F1\n",
    "        clf_df[i,16] = f1_score(y_train, y_pred_train)\n",
    "        clf_df[i,17] = f1_score(y_test, y_pred_test)  \n",
    "        \n",
    "        # Accuracy\n",
    "        clf_df[i,18] = accuracy_score(y_train, y_pred_train)\n",
    "        clf_df[i,19] = accuracy_score(y_test, y_pred_test)\n",
    "        \n",
    "        # AUC\n",
    "        clf_df[i,20] = roc_auc_score(y_train, y_pred_train)\n",
    "        clf_df[i,21] = roc_auc_score(y_test, y_pred_test)\n",
    "        \n",
    "        ### Add X & y values to have everything in one place ### \n",
    "        # These are class balanced/resampled #\n",
    "        clf_df[i,22] = X_train\n",
    "        clf_df[i,23] = X_test\n",
    "        clf_df[i,24] = y_train\n",
    "        clf_df[i,25] = y_test        \n",
    "    \n",
    "    ### Create DataFrame ###\n",
    "    \n",
    "    # Column Names\n",
    "    columns = ['Classifier',\n",
    "               'Model',\n",
    "               'Fitted Model',\n",
    "               'Runtime',\n",
    "               'Train Preds',\n",
    "               'Test Preds',\n",
    "               'Train y-Score',\n",
    "               'Test y-Score',\n",
    "               'Train FPR',\n",
    "               'Train TPR',\n",
    "               'Test FPR',\n",
    "               'Test TPR',\n",
    "               'Train Precision',\n",
    "               'Test Precision',\n",
    "               'Train Recall',\n",
    "               'Test Recall',\n",
    "               'Train F1',\n",
    "               'Test F1',\n",
    "               'Train Accuracy',\n",
    "               'Test Accuracy',\n",
    "               'Train ROC AUC',\n",
    "               'Test ROC AUC',\n",
    "               'X_train',\n",
    "               'X_test',\n",
    "               'y_train',\n",
    "               'y_test'\n",
    "              ]\n",
    "    \n",
    "    # Create DataFrame\n",
    "    clf_df = pd.DataFrame(clf_df, columns=columns)\n",
    "    \n",
    "    return clf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Normalized Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Accuracy Metrics - Plot Confusion Matrices ###\n",
    "\n",
    "# Define function to print out normalized cofusion matrices\n",
    "def norm_cm(clf_df):\n",
    "    '''\n",
    "    Function that prints out normalized confusion matrices\n",
    "    for our classification models.\n",
    "    \n",
    "    Inputs:\n",
    "        clf_df: our DataFrame with out classifier metrics. \n",
    "        \n",
    "    Returns: Prints out normalized confustion matrices for\n",
    "        each of our models in our classifer DataFrame.\n",
    "    '''\n",
    "\n",
    "    for i,clf in clf_df.iterrows():\n",
    "        \n",
    "        classifier = clf['Classifier'] \n",
    "        \n",
    "        # Create Subplots\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,6))\n",
    "        \n",
    "        # Figure Title\n",
    "        fig.suptitle(f\"{classifier}:\")\n",
    "                     \n",
    "        ### Plot ###\n",
    "                     \n",
    "        # Plot Confusion Matrix - Train Set\n",
    "        plot_confusion_matrix(clf['Model'], \n",
    "                              clf['X_train'], \n",
    "                              clf['y_train'], \n",
    "                              cmap='Blues',\n",
    "                              normalize='true',\n",
    "                              ax=ax1)\n",
    "        # Subplot Title\n",
    "        ax1.set_title('Train')\n",
    "        \n",
    "        # Plot Confusion Matrix - Test Set\n",
    "        plot_confusion_matrix(clf['Model'], \n",
    "                              clf['X_test'], \n",
    "                              clf['y_test'], \n",
    "                              cmap='Blues',\n",
    "                              normalize='true',\n",
    "                              ax=ax2)\n",
    "        # Subplot Title\n",
    "        ax2.set_title('Test')\n",
    "              \n",
    "        plt.savefig(f'cm_{classifier}.png', dpi=500, orientation='landscape', bbox_inches='tight')\n",
    "        plt.show()\n",
    "                      \n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Classification Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Accuracy Metrics - Print Classification Reports ###\n",
    "                     \n",
    "# Define function to print out classification reports \n",
    "\n",
    "def clf_reports(clf_df):\n",
    "    '''\n",
    "    Function that prints out classification reports.\n",
    "    \n",
    "    Inputs:\n",
    "        clf_df: our DataFrame with out classifier metrics. \n",
    "        \n",
    "    Returns: Prints out normalized confustion matrices for\n",
    "        each of our models in our classifer DataFrame.\n",
    "    '''\n",
    "    for i,clf in clf_df.iterrows():\n",
    "        \n",
    "        classifier = clf['Classifier']  \n",
    "        print(f\"{classifier}:\\n\")\n",
    "        \n",
    "        print(\"Train:\")\n",
    "        # Print Classification Report - Train\n",
    "        print(classification_report(clf['y_train'], clf['Train Preds']))\n",
    "\n",
    "        print(\"\\nTest:\")\n",
    "        # Print Classification Report - Test\n",
    "        print(classification_report(clf['y_test'], clf['Test Preds']))\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Accuracy Metrics - Plot ROC Curves ###\n",
    "              \n",
    "# Define function to plot ROC curves together\n",
    "\n",
    "def plot_roc(clf_df):\n",
    "    '''\n",
    "    Plots ROC curves for each classifier in DataFrame on\n",
    "    the same plot. Prints AUC value next to classifier name\n",
    "    in plot legend.\n",
    "    \n",
    "    Input: Dataframe containing classifier prediction metrics\n",
    "    \n",
    "    Output: ROC plot\n",
    "    '''\n",
    "    \n",
    "    # Set figure size & ax\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,6))\n",
    "    fig.suptitle('ROC Curves', fontsize=24)    \n",
    "\n",
    "    # Plot each curve\n",
    "    for i,clf in clf_df.iterrows():\n",
    "        \n",
    "        classifier = clf['Classifier']\n",
    "                 \n",
    "        # Training Set \n",
    "        ax1.plot(clf['Train FPR'], \n",
    "                 clf['Train TPR'], \n",
    "                 linestyle='-', \n",
    "                 lw=3,\n",
    "                 label=(str(classifier) + ' - AUC:' + str(\"%0.2f\" % clf['Train ROC AUC']))\n",
    "                )\n",
    "                 \n",
    "        # Sub Title         \n",
    "        ax1.set_title('Train')\n",
    "        ax1.set_xlabel('False Positive Rate')\n",
    "        ax1.set_ylabel('True Positive rate')\n",
    "                        \n",
    "        \n",
    "        # Plot threshold\n",
    "        ax1.plot([0, 1], [0, 1], color='grey', lw=3, linestyle='--')\n",
    "        ax1.set_xlim([-0.05, 1.0])\n",
    "        ax1.set_ylim([0.0, 1.05])\n",
    "        \n",
    "        ax1.legend(loc='lower right')\n",
    "                        \n",
    "                        \n",
    "        # Test Set                \n",
    "        ax2.plot(clf['Test FPR'], \n",
    "                 clf['Test TPR'], \n",
    "                 linestyle='-', \n",
    "                 lw=3, \n",
    "                 label=(str(classifier) + ' - AUC:' + str(\"%0.2f\" % clf['Test ROC AUC']))\n",
    "                )\n",
    "\n",
    "        # Sub Title\n",
    "        ax2.set_title('Test')\n",
    "        ax2.set_xlabel('False Positive Rate')\n",
    "        ax2.set_ylabel('True Positive rate')\n",
    "                 \n",
    "        # Plot threshold\n",
    "        ax2.plot([0, 1], [0, 1], color='grey', lw=3, linestyle='--')\n",
    "        ax2.set_xlim([-0.05, 1.0])\n",
    "        ax2.set_ylim([0.0, 1.05])\n",
    "                        \n",
    "        ax2.legend(loc='lower right')\n",
    "        \n",
    "        # Save fig\n",
    "        plt.savefig(f'roc_curve.png', dpi=500, orientation='landscape', bbox_inches='tight')\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Accuracy Metrics - Accuracy Scores ###\n",
    "              \n",
    "# Function to return table of accuracy scores\n",
    "def acc_scores(clf_preds):\n",
    "    '''\n",
    "    Takes in our DataFrame of classifiers, predictions & metrics.\n",
    "    \n",
    "    Returns a table of accuracy scores as well as runtime of \n",
    "        training each classifier.\n",
    "    '''\n",
    "    \n",
    "    cols = ['Classifier', \n",
    "            'Runtime',\n",
    "            'Train Precision',\n",
    "            'Test Precision', \n",
    "            'Train Recall', \n",
    "            'Test Recall', \n",
    "            'Train F1',\n",
    "            'Test F1', \n",
    "            'Train Accuracy', \n",
    "            'Test Accuracy',\n",
    "            'Train ROC AUC', \n",
    "            'Test ROC AUC', \n",
    "           ]\n",
    "    \n",
    "    acc_scores = clf_preds[cols]\n",
    "    acc_scores = acc_scores.set_index('Classifier')\n",
    "    \n",
    "    # Round values \n",
    "    acc_scores = acc_scores.astype(float).round(2)\n",
    "    \n",
    "    # Save to file\n",
    "    acc_scores.to_csv(\"Accuracy Scores.csv\")\n",
    "    \n",
    "    return acc_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance Plot\n",
    "\n",
    "For XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Feature Importance Plot - XGBoost ###\n",
    "              \n",
    "# Create a function to output our feature importance plot\n",
    "def feat_rank(xgb_clf,X):\n",
    "    '''\n",
    "    Input XGBoost fitted classifier and X: DataFrame of features \n",
    "    \n",
    "    Returns ranked feature importance plot\n",
    "    '''\n",
    "\n",
    "    # We want to add back our column names to our feature importances\n",
    "    ft_rank = pd.Series(xgb_clf.feature_importances_, index = X.columns)\n",
    "\n",
    "    # Sort ascending\n",
    "    ft_rank = ft_rank.sort_values(ascending = True)\n",
    "\n",
    "    # Plot\n",
    "    ft_rank.plot(kind='barh', width=.75, edgecolor = \"black\", figsize=(12,10), fontsize=18)\n",
    "    plt.title('Feature Importance Ranking - XGBoost', fontsize=22)\n",
    "    \n",
    "    # Save fig\n",
    "    plt.savefig(f'feat_rank_XGB.png', dpi=500, orientation='landscape', bbox_inches='tight')\n",
    "    \n",
    "    \n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Grid Search ###\n",
    "# Define Function to Conduct Grid Search\n",
    "def grid_search(params, clf_df, cv=3, scoring='recall'):\n",
    "    '''\n",
    "    Conducts gridsearch using GridSearchCV\n",
    "    \n",
    "    Inputs:\n",
    "        params: parameter grid in GridSearchCV dictionary format\n",
    "        clf: classifier \n",
    "        cv: k-folds for crossvalidation\n",
    "        scoring: scoring metric for optimization\n",
    "            'accuracy','recall','precision','f1'\n",
    "            \n",
    "    Returns:\n",
    "        Best score for training accuracy\n",
    "        Optimal parameters for model\n",
    "    '''\n",
    "    for i, clf in clf_df.iterrows():\n",
    "        print(clf['Classifier'])\n",
    "        print(f\"\\nParameter Grid: {params[i]} \\n\")\n",
    "        clf_grid_search = GridSearchCV(clf['Model'],\n",
    "                                  param_grid=params[i],\n",
    "                                  cv=cv,\n",
    "                                  scoring=scoring)\n",
    "        clf_grid_search.fit(clf['X_train'], clf['y_train'])\n",
    "        print(f\"Training Accuracy: {clf_grid_search.best_score_ :.2%} \\n\")\n",
    "        print(f\"Optimal Parameters: {clf_grid_search.best_params_} \\n\\n\")\n",
    "        \n",
    "    return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
